{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T17:06:34.439508Z",
     "start_time": "2020-06-25T17:06:34.435098Z"
    }
   },
   "source": [
    "## Importando dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de uma análise exploratória de dados, pré-processamento de um projeto de machine learning, o primeiro passo para ser executado é a importação dos dados a serem utilizados. Como utilizaremos uma conotação matricial para este processo, usa-se de ocstume prático a biblioteca Pandas para este processo. Na célula inferior o código é inserido as biblio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import pandas as pandas\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import (model_selection,metrics,svm)\n",
    "from sklearn.preprocessing import (StandardScaler, LabelEncoder)\n",
    "from sklearn.model_selection import (train_test_split,KFold,cross_val_score,GridSearchCV)\n",
    "from sklearn.metrics import (confusion_matrix,classification_report,accuracy_score)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data importing\n",
    "data = pd.read_csv('file_name')\n",
    "data.shape\n",
    "data.head()\n",
    "data[y_target].value_count() # Balance database\n",
    "X = daata.drop(labels=y_target,axis=1)\n",
    "Y = data[y_target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "X.isnull().sum()\n",
    "df_norm = (X - X.mean())/ (X.max() - X.min())\n",
    "df_norm = pd.concat([df_norm,Y],axis=1)\n",
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "# Correlation\n",
    "plt.rcParams['figure.figsize']=(12,8)\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(data.drop(y_target, axis=1).corr(),cmap='coolwarm'))\n",
    "\n",
    "# Outliers\n",
    "f, (axi) = plt.subplots(1,5)\n",
    "s.boxplot(y_target, y=X[column[i]],data=data, ax=axi)\n",
    "f.tight_layout\n",
    "\n",
    "# Distribution\n",
    "g = s.FazetGrid(data,col=y_target, hue=y_target)\n",
    "g.map(sns.displot,X[column[i]],hist=True, rug=True)\n",
    "\n",
    "#Smoothness\n",
    "f, (axi) = plt.subplots(1,5)\n",
    "s.boxplot(y_target, y=X[column[i]],data=data, ax=axi,palette='cubehelix')\n",
    "f.tight_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and Encoding\n",
    "X_norm = df_norm.drop(labels=y_target,axis=1)\n",
    "Y_norm = df_norm[y_target]\n",
    "col = X_norm.columns\n",
    "le = LabelEncoder()\n",
    "le.fit('categorical data')\n",
    "Y_norm = le.transform(Y_norm)\n",
    "Y_norm = pd.DataFrame(Y_norm)\n",
    "Y_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FitModel(X,Y,algo_name,algorithm,gridSearchParams,cv):\n",
    "    np.random.seed(10)\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,Y, test_size = 0.2)\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=algorithm,\n",
    "        param_grid=gridSearchParams,\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        verbose=1,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    grid_result = grid.fit(x_train, y_train)\n",
    "    best_params = grid_resultbest_params_\n",
    "    pred = grid_result.predict(x_test)\n",
    "    cm = confusion_matrix(y_test,pred)\n",
    "    metrics = grid_result.gr\n",
    "\n",
    "    print('Best params :', best_params)\n",
    "    print('Classification Report :', classification_report(y_test,pred))\n",
    "    print('Accuracy Score :', str(accuracy_score(y_test,pred)))\n",
    "    print('Confusion Matrix : \\n', cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC ML Model\n",
    "\n",
    "params = {\n",
    "    'C': [0.1,1,100,1000],\n",
    "    'gamma': [0.0001,0.001,00.5,0.1,1,3,5]\n",
    "}\n",
    "\n",
    "FitModel(X_norm,Y_norm,'SVC',SVC(),param,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest ML Model\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [100,500,1000,2000]\n",
    "}\n",
    "\n",
    "FitModel(X_norm,Y_norm,'Random Forest',RandomForestClassifier(),param,cv=10)\n",
    "\n",
    "# TEST for feature importance\n",
    "np.random.seed(10)\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y, test_size = 0.2)\n",
    "forest = RandomForestClassifier(n_estimators=1000)\n",
    "fit = forest.fit(x_train,y_train)\n",
    "accuracy = fit.score(x_test,y_test)\n",
    "predict = fit.predict(x_test)\n",
    "cmatrix = confusion_matrix(y_test,predict)\n",
    "\n",
    "#\n",
    "importances = forest.feature_importance_\n",
    "indices = np.argsort(importances)[::1]\n",
    "\n",
    "print('Feature ranking:')\n",
    "for f in range(X.shape[1]):\n",
    "    print(f'feature {list(x)[f]} {importances[indices[f]]}')\n",
    "\n",
    "feat_imp = pd.DataFrame({\n",
    "    'Feature':list(X),\n",
    "    'Gini importance': impotances[indices]\n",
    "})\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12,12)\n",
    "sns.set_style('whitegrid')\n",
    "ax = sns.barplot(x='Gini importance', y='Feature', data=feat_imp)\n",
    "ax.set(xlabel='Gini importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [100,500,1000,2000]\n",
    "}\n",
    "\n",
    "FitModel(X,Y,'XGBoost',XGBClassifer(),param,cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing Dataset\n",
    "# Check target difference classes\n",
    "\n",
    "# oversampling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
